# -*- coding: utf-8 -*-
"""dl_YOLOS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ouyz_y7wdF8tx8xevNPB-XubHvGzrHt7
"""

# CUDA: Let's check that Nvidia CUDA drivers are already pre-installed and which version is it.
!/usr/local/cuda/bin/nvcc --version

!ls /usr/bin | grep nvidia
!echo $PATH

!nvidia-smi

"""Add the dataset"""

!pip install --upgrade dataset-tools

!pip install -q transformers==4.20.0
!pip install -q pytorch-lightning

import dataset_tools as dtools
dtools.download(dataset='deepNIR Fruit Detection', dst_dir='/content/dataset-ninja/')

"""Check the structure of this dataset"""

!ls /content/dataset-ninja/deepnir-fruit-detection

import os
import json
import shutil
from sklearn.model_selection import train_test_split

# Define paths
train_path = "/content/dataset-ninja/deepnir-fruit-detection/train"
val_path = "/content/dataset-ninja/deepnir-fruit-detection/valid"
test_path = "/content/dataset-ninja/deepnir-fruit-detection/test"

# List the contents of the img and ann directories
train_img_files = os.listdir(os.path.join(train_path, "img"))
train_ann_files = os.listdir(os.path.join(train_path, "ann"))
val_img_files = os.listdir(os.path.join(val_path, "img"))
val_ann_files = os.listdir(os.path.join(val_path, "ann"))
test_img_files = os.listdir(os.path.join(test_path, "img"))
test_ann_files = os.listdir(os.path.join(test_path, "ann"))

print("Number of train images:", len(train_img_files))
print("Number of train annotations:", len(train_ann_files))
print("Number of val images:", len(val_img_files))
print("Number of val annotations:", len(val_ann_files))
print("Number of test images:", len(test_img_files))
print("Number of test annotations:", len(test_ann_files))

# Create directories if not exist
os.makedirs(os.path.join(train_path, "img"), exist_ok=True)
os.makedirs(os.path.join(train_path, "ann"), exist_ok=True)
os.makedirs(os.path.join(val_path, "img"), exist_ok=True)
os.makedirs(os.path.join(val_path, "ann"), exist_ok=True)
os.makedirs(os.path.join(test_path, "img"), exist_ok=True)
os.makedirs(os.path.join(test_path, "ann"), exist_ok=True)

# discover the "img" and "ann"
# Print the first five files in each directory
print("First five files in 'img':", train_img_files[:5])
print("First five files in 'ann':", train_ann_files[:5])

"""We found that it's not sorted, so we need to pair them up manually: ann name(.jpg) = file name.jpg(.json)

Also filter to have only bounding boxes for the wheat class
"""

import matplotlib.pyplot as plt
import matplotlib.patches as patches
from PIL import Image

def filter_bounding_boxes(img_dir, ann_dir, visualize=False):
    """
    Filters images and annotations to include only those with bounding boxes
    for the specified class.

    Args:
        img_dir (str): Directory containing image files.
        ann_dir (str): Directory containing annotation files.
        visualize (bool): If True, displays bounding boxes on the image.

    Returns:
        list: Filtered image file paths.
        list: Filtered annotation file paths.
    """
    filtered_imgs = []
    filtered_anns = []

    for img_file in sorted(os.listdir(img_dir)):
        img_path = os.path.join(img_dir, img_file)
        ann_file = img_file + ".json"  # Match image file
        ann_path = os.path.join(ann_dir, ann_file)

        with open(ann_path, 'r') as f:
            annotation = json.load(f)

        # Check if the annotation contains at least one bounding box for the class
        contains_class = any(
            obj["classTitle"] == "wheat" and obj["geometryType"] == "rectangle"
            for obj in annotation.get("objects", [])
        )

        if contains_class:
            filtered_imgs.append(img_path)
            filtered_anns.append(ann_path)

            # Visualization (if enabled)
            if visualize:
                print(f"Visualizing: {img_path}")
                bounding_boxes = [
                    obj["points"]["exterior"]
                    for obj in annotation["objects"]
                    if obj["classTitle"] == "wheat" and obj["geometryType"] == "rectangle"
                ]

                # Open and display the image with bounding boxes
                img = Image.open(img_path).convert("RGB")
                fig, ax = plt.subplots(1, 1, figsize=(10, 10))
                ax.imshow(img)

                # Add bounding boxes
                for bbox in bounding_boxes:
                    x_min, y_min = bbox[0]
                    x_max, y_max = bbox[1]
                    width = x_max - x_min
                    height = y_max - y_min
                    rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='red', facecolor='none')
                    ax.add_patch(rect)

                ax.set_title(f"Bounding Boxes for {img_path}")
                plt.axis('off')
                plt.show()

            # Extract detected object
            detected_objects = [obj["classTitle"] for obj in annotation.get("objects", [])]
            # print(f"Object Class is: {detected_objects[0]}")

            # Count the number of detected objects
            num_objects = len(detected_objects)
            print(f"{img_file}: Object Class is {detected_objects[0]}; Num detected: {num_objects}")

    print(f"Filtered {len(filtered_imgs)} images and annotations.")

    return filtered_imgs, filtered_anns

train_imgs, train_anns = filter_bounding_boxes(os.path.join(train_path, "img"), os.path.join(train_path, "ann"))
val_imgs, val_anns = filter_bounding_boxes(os.path.join(val_path, "img"), os.path.join(val_path, "ann"))
test_imgs, test_anns = filter_bounding_boxes(os.path.join(test_path, "img"), os.path.join(test_path, "ann"))

def save_filtered_data(img_paths, ann_paths, img_dest, ann_dest):
    """
    Saves filtered images and annotations into specified directories.

    Args:
        img_paths (list): List of image file paths.
        ann_paths (list): List of annotation file paths.
        img_dest (str): Destination directory for images.
        ann_dest (str): Destination directory for annotations.
    """
    os.makedirs(img_dest, exist_ok=True)
    os.makedirs(ann_dest, exist_ok=True)

    for img_path, ann_path in zip(img_paths, ann_paths):
        shutil.copy(img_path, img_dest)
        shutil.copy(ann_path, ann_dest)

# Paths for saving filtered data
filtered_train_path = "/content/dataset-ninja/wheat/train"
filtered_val_path = "/content/dataset-ninja/wheat/val"
filtered_test_path = "/content/dataset-ninja/wheat/test"

# Save filtered datasets
save_filtered_data(train_imgs, train_anns, os.path.join(filtered_train_path, "img"), os.path.join(filtered_train_path, "ann"))
save_filtered_data(val_imgs, val_anns, os.path.join(filtered_val_path, "img"), os.path.join(filtered_val_path, "ann"))
save_filtered_data(test_imgs, test_anns, os.path.join(filtered_test_path, "img"), os.path.join(filtered_test_path, "ann"))

# Final counts
print(f"Filtered Training set: {len(train_imgs)} images, {len(train_anns)} annotations")
print(f"Filtered Validation set: {len(val_imgs)} images, {len(val_anns)} annotations")
print(f"Filtered Test set: {len(test_imgs)} images, {len(test_anns)} annotations")

"""## Convert data to COCO format"""

import os
import json
from PIL import Image

def convert_to_coco_format(images_dir, annotations_dir, output_file):
  coco_format = {
      "images": [],
      "annotations": [],
      "categories": []
  }

  # Define categories (modify as per your dataset)
  categories = [
    {"id": 0, "name": "wheat"},
  ]

  # Add categories to COCO format
  coco_format["categories"] = categories

  annotation_id = 0

  # Process each annotation file
  for ann_file in os.listdir(annotations_dir):
    if ann_file.endswith(".json"):
        ann_path = os.path.join(annotations_dir, ann_file)
        image_name = os.path.splitext(ann_file)[0]
        image_path = os.path.join(images_dir, image_name)

        # Skip if the image does not exist
        if not os.path.exists(image_path):
            continue

        # Get image dimensions
        with Image.open(image_path) as img:
          width, height = img.size

        # Add image entry
        image_id = len(coco_format["images"]) + 1
        coco_format["images"].append({
            "id": image_id,
            "file_name": image_name,
            "width": width,
            "height": height
        })

        # Load annotation file
        with open(ann_path, "r") as f:
            annotations = json.load(f)

        # Add annotations to COCO format
        for obj in annotations.get("objects", []):
            x_min, y_min = obj["points"]["exterior"][0]
            x_max, y_max = obj["points"]["exterior"][1]
            box_width = (x_max - x_min)
            box_height = (y_max - y_min)
            bbox = [x_min, y_min, box_width, box_height]

            class_id = obj["classId"]

            # COCO format: [x, y, width, height]
            coco_format["annotations"].append({
                "id": annotation_id,
                "image_id": image_id,
                "category_id": 0,
                "bbox": bbox,
                "area": box_width * box_height,
                "iscrowd": 0
            })

            annotation_id += 1

  # Save the COCO annotations to a JSON file
  with open(output_file, "w") as f:
    json.dump(coco_format, f, indent=4)

  print(f"COCO annotations saved to {output_file}")

coco_train_output = os.path.join(filtered_train_path, "coco_annotations_train.json")
coco_test_output = os.path.join(filtered_test_path, "coco_annotations_test.json")
coco_val_output = os.path.join(filtered_val_path, "coco_annotations_val.json")

convert_to_coco_format(
    images_dir=os.path.join(filtered_train_path, "img"),
    annotations_dir=os.path.join(filtered_train_path, "ann"),
    output_file=coco_train_output
)

convert_to_coco_format(
    images_dir=os.path.join(filtered_test_path, "img"),
    annotations_dir=os.path.join(filtered_test_path, "ann"),
    output_file=coco_test_output
)

convert_to_coco_format(
    images_dir=os.path.join(filtered_val_path, "img"),
    annotations_dir=os.path.join(filtered_val_path, "ann"),
    output_file=coco_val_output
)

! mkdir data
! mkdir data/images
! mkdir data/annotations

! mv /content/dataset-ninja/wheat/train/img data/images/train
! mv /content/dataset-ninja/wheat/test/img data/images/test
! mv /content/dataset-ninja/wheat/val/img data/images/val

! cp /content/dataset-ninja/wheat/train/coco_annotations_train.json data/annotations/
! cp /content/dataset-ninja/wheat/test/coco_annotations_test.json data/annotations/
! cp /content/dataset-ninja/wheat/val/coco_annotations_val.json data/annotations/

from google.colab import drive
drive.mount('/content/drive')

!zip -r data.zip data
!cp data.zip /content/drive/MyDrive/data.zip

#!cp /content/drive/MyDrive/data.zip ./
#!unzip data.zip

"""## Load Dataset"""

#Register dataset as torchvision CocoDetection
import torchvision
import os

class CocoDetection(torchvision.datasets.CocoDetection):
    def __init__(self, img_folder, ann_file, feature_extractor, train=True):
        super(CocoDetection, self).__init__(img_folder, ann_file)
        self.feature_extractor = feature_extractor

    def __getitem__(self, idx):
        # read in PIL image and target in COCO format
        img, target = super(CocoDetection, self).__getitem__(idx)

        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)
        image_id = self.ids[idx]
        target = {'image_id': image_id, 'annotations': target}
        encoding = self.feature_extractor(images=img, annotations=target, return_tensors="pt")
        pixel_values = encoding["pixel_values"].squeeze() # remove batch dimension
        target = encoding["labels"][0] # remove batch dimension

        return pixel_values, target

from transformers import AutoFeatureExtractor

image_path = "/content/data/images"
ann_path = "/content/data/annotations"

feature_extractor = AutoFeatureExtractor.from_pretrained("hustvl/yolos-small", size=512, max_size=864)

train_dataset = CocoDetection(img_folder=(image_path+"/train"), ann_file=(ann_path+"/coco_annotations_train.json"), feature_extractor=feature_extractor)
val_dataset = CocoDetection(img_folder=(image_path+"/val"), ann_file=(ann_path+"/coco_annotations_val.json"), feature_extractor=feature_extractor)
test_dataset = CocoDetection(img_folder=(image_path+"/test"), ann_file=(ann_path+"/coco_annotations_test.json"), feature_extractor=feature_extractor)

print("Number of training examples:", len(train_dataset))
print("Number of validation examples:", len(val_dataset))
print("Number of test examples:", len(test_dataset))

#Vizualize that our data has loaded correctly - You can hit this cell as many times as you want to vizualize how your training set has loaded
import numpy as np
import os
from PIL import Image, ImageDraw

# based on https://github.com/woctezuma/finetune-detr/blob/master/finetune_detr.ipynb
image_ids = train_dataset.coco.getImgIds()
# let's pick a random image
image_id = image_ids[np.random.randint(0, len(image_ids))]
print('Image n°{}'.format(image_id))
imageInfo = train_dataset.coco.loadImgs(image_id)[0]
image = Image.open(os.path.join(image_path + '/train', imageInfo['file_name']))

print(image_id, os.path.join(image_path + '/train', imageInfo['file_name']))

annotations = train_dataset.coco.imgToAnns[image_id]
draw = ImageDraw.Draw(image, "RGBA")

cats = train_dataset.coco.cats
id2label = {k: v['name'] for k,v in cats.items()}

for annotation in annotations:
  box = annotation['bbox']
  class_idx = annotation['category_id']
  x,y,w,h = tuple(box)
  draw.rectangle((x,y,x+w,y+h), outline='red', width=1)
  draw.text((x, y), id2label[class_idx], fill='white')

image

"""# Train YOLO"""

#Setup dataloader for training loop

from torch.utils.data import DataLoader

def collate_fn(batch):
  pixel_values = [item[0] for item in batch]
  encoding = feature_extractor.pad(pixel_values, return_tensors="pt")
  labels = [item[1] for item in batch]
  batch = {}
  batch['pixel_values'] = encoding['pixel_values']
  batch['labels'] = labels
  return batch

train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=4, shuffle=True, num_workers=7)
val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=4, num_workers=7)
batch = next(iter(train_dataloader))

# For our training configuration, we will start from the pretrained YOLOS-tiny model from
# Hugging Face transformers

import pytorch_lightning as pl
from transformers import DetrConfig, AutoModelForObjectDetection
import torch
from torch.optim.lr_scheduler import CosineAnnealingLR

#we wrap our model around pytorch lightning for training
class YoloS(pl.LightningModule):

     def __init__(self, lr, weight_decay):
         super().__init__()
         # replace COCO classification head with custom head
         self.model = AutoModelForObjectDetection.from_pretrained("hustvl/yolos-tiny",
                                                             num_labels=len(id2label),
                                                             ignore_mismatched_sizes=True)
         # see https://github.com/PyTorchLightning/pytorch-lightning/pull/1896
         self.lr = lr
         self.weight_decay = weight_decay
         self.save_hyperparameters()  # adding this will save the hyperparameters to W&B too

     def forward(self, pixel_values):
       outputs = self.model(pixel_values=pixel_values)

       return outputs

     def common_step(self, batch, batch_idx):
       pixel_values = batch["pixel_values"]
       labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch["labels"]]

       outputs = self.model(pixel_values=pixel_values, labels=labels)

       loss = outputs.loss
       loss_dict = outputs.loss_dict

       return loss, loss_dict

     def training_step(self, batch, batch_idx):
        loss, loss_dict = self.common_step(batch, batch_idx)
        # logs metrics for each training_step,
        # and the average across the epoch
        self.log("train/loss", loss)  # logging metrics with a forward slash will ensure the train and validation metrics as split into 2 separate sections in the W&B workspace
        for k,v in loss_dict.items():
          self.log("train/" + k, v.item())  # logging metrics with a forward slash will ensure the train and validation metrics as split into 2 separate sections in the W&B workspace

        return loss

     def validation_step(self, batch, batch_idx):
        loss, loss_dict = self.common_step(batch, batch_idx)
        self.log("validation/loss", loss) # logging metrics with a forward slash will ensure the train and validation metrics as split into 2 separate sections in the W&B workspace
        for k,v in loss_dict.items():
          self.log("validation/" + k, v.item()) #  logging metrics with a forward slash will ensure the train and validation metrics as split into 2 separate sections in the W&B workspace

        return loss

     def configure_optimizers(self):
        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr,
                                  weight_decay=self.weight_decay)
        # return optimizer

        # Define scheduler
        scheduler = CosineAnnealingLR(optimizer, T_max=10)
        return [optimizer], [scheduler]


     def train_dataloader(self):
        return train_dataloader

     def val_dataloader(self):
        return val_dataloader

#initialize the model
from pytorch_lightning import Trainer

# # Path to the checkpoint file
# checkpoint_path = "drive/MyDrive/logs/epoch20/version_0/checkpoints/epoch=19-step=1700.ckpt"

# # Load the model from the checkpoint
# model = YoloS.load_from_checkpoint(checkpoint_path)

model = YoloS(lr=2.5e-5, weight_decay=1e-4)

from pytorch_lightning import Trainer
from pytorch_lightning.loggers import CSVLogger
from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint

# Define callbacks
early_stopping = EarlyStopping(
    monitor="validation/loss", patience=3, mode="min"
)

checkpoint_callback = ModelCheckpoint(
    dirpath="drive/MyDrive/checkpoints/",
    filename="best_model",
    save_top_k=1,
    monitor="validation/loss",
    mode="min"
)

# csv_logger = CSVLogger("logs/", name="my_model")
csv_logger = CSVLogger("/content/drive/MyDrive/logs/", name="epoch40-changed")

# more epochs leads to a tighter fit of the model to the data
trainer = Trainer(
    max_epochs=40,
    gradient_clip_val=0.1,
    accumulate_grad_batches=8,
    log_every_n_steps=5,
    logger=csv_logger,
    callbacks=[early_stopping, checkpoint_callback]
    )

trainer.fit(model)

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Generate predictions on the test dataset
test_predictions = []
model.eval()  # Set model to evaluation mode
with torch.no_grad():
    for idx in range(len(test_dataset)):
        pixel_values, target = test_dataset[idx]
        pixel_values = pixel_values.unsqueeze(0).to(device)  # Use the defined device
        outputs = model(pixel_values=pixel_values)
        test_predictions.append(outputs)

import torch

# Convert bounding boxes from (center_x, center_y, width, height) to (x_min, y_min, x_max, y_max)
def box_cxcywh_to_xyxy(box):
    x_c, y_c, w, h = box.unbind(-1)
    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]
    return torch.stack(b, dim=-1)

# Rescale bounding boxes from normalized format to pixel format
def rescale_bboxes(boxes, image_size):
    img_w, img_h = image_size
    boxes = box_cxcywh_to_xyxy(boxes)  # Convert to (x_min, y_min, x_max, y_max) format
    scaling_tensor = torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32, device=boxes.device)
    boxes = boxes * scaling_tensor
    return boxes

# Converts model predictions to COCO-style annotations for evaluation
def convert_predictions_to_coco(predictions, image_id, image_size, confidence_threshold=0.5):
    logits = predictions.logits[0]
    probabilities = logits.softmax(dim=-1)
    pred_boxes = predictions.pred_boxes[0]

    # Filter by confidence threshold
    keep = probabilities.max(-1).values > confidence_threshold
    filtered_boxes = pred_boxes[keep]
    filtered_probs = probabilities[keep]

    # Convert to COCO format
    rescaled_boxes = rescale_bboxes(filtered_boxes, image_size)  # Rescale to pixel coordinates
    coco_predictions = []
    for box, prob in zip(rescaled_boxes, filtered_probs):
        class_id = prob.argmax().item()
        confidence = prob[class_id].item()
        coco_predictions.append({
            "image_id": image_id,
            "category_id": class_id,
            "bbox": [box[0].item(), box[1].item(), box[2].item() - box[0].item(), box[3].item() - box[1].item()],
            "score": confidence
        })
    return coco_predictions

import json

with open('/content/data/annotations/coco_annotations_test.json', 'r') as f:
    ground_truth = json.load(f)

# Map by image_id
ground_truth_by_image = {}
for ann in ground_truth['annotations']:
    ground_truth_by_image.setdefault(ann['image_id'], []).append(ann)

# Prepare predictions for all images
predictions = []
for idx, outputs in enumerate(test_predictions):
    image_id = test_dataset.coco.getImgIds()[idx]
    image_info = test_dataset.coco.loadImgs(image_id)[0]
    image_size = (image_info['width'], image_info['height'])
    predictions += convert_predictions_to_coco(outputs, image_id, image_size)

!pip install pycocotools

from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

coco_gt = COCO('/content/data/annotations/coco_annotations_test.json')

# Load predictions into COCO API format
coco_dt = coco_gt.loadRes(predictions)

# Initialize COCOeval
coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')

# Run evaluation
coco_eval.evaluate()
coco_eval.accumulate()
coco_eval.summarize()

"""## Visualizing Inference"""

#We can use the image_id in target to know which image it is
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

pixel_values, target = test_dataset[1]
pixel_values = pixel_values.unsqueeze(0).to(device)
outputs = model(pixel_values=pixel_values)

import torch
import matplotlib.pyplot as plt

#lower confidence yields more, but less accurate predictions
CONFIDENCE=0.2

# colors for visualization
COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],
          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]

# for output bounding box post-processing
def box_cxcywh_to_xyxy(x):
    x_c, y_c, w, h = x.unbind(1)
    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),
         (x_c + 0.5 * w), (y_c + 0.5 * h)]
    return torch.stack(b, dim=1)

def rescale_bboxes(out_bbox, size):
    img_w, img_h = size
    b = box_cxcywh_to_xyxy(out_bbox)
    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)
    return b

def plot_results(pil_img, prob, boxes):
    plt.figure(figsize=(16,10))
    plt.imshow(pil_img)
    ax = plt.gca()
    colors = COLORS * 100
    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):
        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,
                                   fill=False, color=c, linewidth=3))
        cl = p.argmax()
        text = f'{id2label[cl.item()]}: {p[cl]:0.2f}'
        ax.text(xmin, ymin, text, fontsize=15,
                bbox=dict(facecolor='yellow', alpha=0.5))
    plt.axis('off')
    plt.show()

def visualize_predictions(image, outputs, threshold=CONFIDENCE):
  # keep only predictions with confidence >= threshold
  probas = outputs.logits.softmax(-1)[0, :, :-1]
  keep = probas.max(-1).values > threshold

  # convert predicted boxes from [0; 1] to image scales
  bboxes_scaled = rescale_bboxes(outputs.pred_boxes[0, keep].cpu(), image.size)

  # plot results
  plot_results(image, probas[keep], bboxes_scaled)

image_id = target['image_id'].item()
image = test_dataset.coco.loadImgs(image_id)[0]
image = Image.open(os.path.join(image_path + '/test', image['file_name']))

visualize_predictions(image, outputs)